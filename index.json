[{"categories":["Azure","Azure AD","Managed Identity","Identity","Users","Groups","Privileges","Cloud"],"content":"Hoy me gustaría hablaros un poco sobre Azure Active Directory (Azure AD) y explicaros algunos conceptos como gestionar las famosas Managed Identity, desde un perspectiva general de su uso.\n##¿Qué es Azure Active Directory? Azure Active Directory (también conocido como Azure AD) es un servicio multi-inquilino completamente administrado de Microsoft que ofrece capacidades de identidad y acceso para aplicaciones que se ejecutan en Microsoft Azure y para aplicaciones que se ejecutan en un entorno local. Esto hace que a menudo pueda ser confundido con Active Directory para Windows Server, en realidad esto va mucho más allá de lo que ya estamos familiarizados en entornos Windows. Para resumirlo, no, Azure AD no es Active Directory ejecutándose en máquinas virtuales de Azure.\nAdemás con Azure AD podemos lograr cosas como el inicio de sesión único (SSO) o multifactor, acceso a aplicaciones aplicaciones, consultar los permisos e información de usuarios y grupos, e incluso escribir cambios en el directorio y la mejor parte es que puede integrar Azure AD con Windows Server AD.\nAzure AD es un lugar donde se crearán los usuarios y los perfiles. Por lo tanto, los usuarios o empleados de una organización iniciarán sesión con su nombre de usuario y contraseña. Y, por supuesto, a veces puede requerir autenticación multifactor. Y luego, basándonos en esa identidad, proporcionamos acceso a las aplicaciones.\n##Autenticación frente a autorización Antes de continuar, primero me gustaría poder discernir entre dos conceptos que tienden a confundirse; la diferencia entre la autenticación y autorización. Puede que ya lo sepas. Pero de todos modos lo explicaré rápidamente.\nEl proceso de identificar a alguien a sí mismo se llama autenticación. Es como si vamos por la calle y nos piden que nos identifiquemos - ¡Alto, Policía!, lo que seguramente haremos es mostrar nuestro DNI, Pasaporte o licencia de conducir. Fácil ¿verdad?\nEntonces, ¿qué es la autorización?\nSi tomamos el ejemplo anterior. Ya hemos demostrado nuestra identidad; la siguiente pregunta inmediata es ¿qué puedo hacer o qué no puedo hacer con esa identidad en una organización o sistema en particular? ¿Estoy en el lugar correcto o me he colado?\nDisponer de la autorización oportuna nos permite llegar todo lo lejos que nos permita la autoridad que otorga ese derecho. Siguiendo con nuestro ejemplo, en un concierto, ¿tenemos una entrada que nos permite acceder al recinto o disponemos de un pase VIP que nos da acceso al backstage?\nEn el contexto de Azure, Autorización es poder responder a la pregunta ¿Cuáles son los diferentes servicios a los que puedo acceder y a los que no tengo acceso?\n##Autenticación multifactor Ahora hablemos un poco de la autenticación multifactor. Estoy seguro que es un término familiar que ya lo has oído antes. Cuando iniciamos sesión en un sitio web en particular, proporcionamos nuestro nombre de usuario y contraseña. Imagina que por alguna razón nuestra contraseña se filtra. Entonces alguien sin acceso autorizado y que tenga nuestra contraseña puede acceder a los datos y servicios que contiene.\nEntonces, ¿cómo podemos mitigar este problema? Ahí es cuando necesitamos la autenticación multifactor (MFA). Además del nombre de usuario y la contraseña, también se debe proporcionar la identidad en forma de probablemente una OTP que obtenemos en nuestro teléfono móvil o mediante una llamada telefónica o usando una aplicación móvil instalada en donde confirmamos que somos nosotros los que estamos haciendo inicio de sesión.\nLa autenticación multifactor funciona al requerir dos o más de los siguientes métodos de autenticación:\n  Algo que sepas, normalmente una contraseña.\n  Algo que tenga, como un dispositivo confiable que no se pueda duplicar fácilmente. (Por ejemplo: teléfono o llave de hardware)\n  Algo que eres: datos biométricos como una huella dactilar o un escaneo facial.\n  ##¿Qué es una identidad administrada? Las identidades administradas proporcionan una identidad para que las aplicaciones la usen cuando se conectan a recursos que admiten la autenticación de Azure Active Directory (Azure AD). Fundamentalmente, la gestión de credenciales es manejada por la identidad administrada (de ahí la palabra), y no por la aplicación o el desarrollador.\nDicho de otro modo, la aplicación usa la identidad administrada para obtener un token de Azure AD. Este token se puede usar para acceder a otros recursos de Azure, comúnmente Azure Key Vault o Azure Storage.\nInicio de sesión único (SSO)\nLa misma identidad se puede utilizar en varias aplicaciones. Y esa función se denomina inicio de sesión único. Inicia sesión una vez, y con ese inicio de sesión en particular, intenta acceder a todos los recursos.\nGestión de aplicaciones\nComo mencioné antes, cuando creamos y configuramos múltiples aplicaciones para Azure AD. Estas aplicaciones solo pueden ser utilizadas por usuarios dentro de nuestra organización o, en ciertos casos, por algunos usuarios invitados pertenecientes a otra organización. Por lo tanto, podemos invitar a usuarios de otra organización. Eso significa que hay un inquilino AD más donde existe la identidad del usuario. Pero en nuestro AD, va a existir como invitado.\nUna vez que el usuario invitado se agrega a nuestro Directorio Activo, junto con nuestros otros usuarios, a este usuario en particular también se le pueden otorgar permisos como cualquier otro usuario dentro de la misma organización.\nGestión de dispositivos\nYa por último, podemos proporcionar administración de dispositivos mediante Azure AD uniendo nuestros dispositivos móviles u ordenadores portátiles al inquilino de Azure. A través del inquilino, podemos controlar el dispositivo. Un ejemplo práctico es cuando pierdes tu dispositivo y luego puedes bloquear tu cuenta o más concretamente el acceso mediante ese dispositivo. Por lo tanto, en el dispositivo, nadie más puede iniciar sesión y robar tus datos.\n##Agregando usuarios a Azure AD Tras las explicaciones previas, ahora vamos a agregar nuestro primer usuario. Para ello debemos seguir los siguientes pasos:\nInicia sesión en Azure Portal como administrador de usuarios para tu organización.\nEn la barra superior, busca y selecciona Azure Active Directory.\nSelecciona Usuarios y, a continuación, selecciona Nuevo usuario.\nEn la página Usuario puedes definir la información relativa para este usuario:\n  Nombre. Nombre y apellidos del nuevo usuario. Por ejemplo, Mary Poppins.\n  Nombre de usuario. Nombre de usuario del nuevo usuario. Por ejemplo, mary@contoso.com. En la parte del dominio del nombre de usuario debemos usar el nombre de dominio predeterminado inicial, .onmicrosoft.com o un nombre de dominio personalizado, como de contoso.com.\n  Grupos. Podemos agregar el usuario a uno o varios de los grupos existentes.\n  Rol del directorio. Aquí podemos definir alguno de los permisos administrativos de Azure AD para el usuario\n  Información del trabajo. Podemos agregar más información sobre el usuario aquí o hacerlo más adelante.\n  Copia la contraseña generada automáticamente proporcionada en el cuadro de texto Contraseña. Esta será la contraseña que debemos proporcionar al usuario para iniciar sesión por primera vez.\n  Selecciona Crear. El usuario queda así definido y se agrega a la organización de Azure AD.\nTambién podemos invitar a un usuario invitado nuevo a colaborar con nuestra organización si selecciona Invitar usuario en la página Nuevo usuario. El usuario recibirá una invitación por correo electrónico que debe aceptar para empezar a colaborar.\n##Asignación de roles Lo comentaba antes pero quizá es mejor dedicarle unas líneas adicionales. En Azure es posible administrar usuarios asignándoles alguno de los diferentes roles disponibles. En Azure Active Directory (Azure AD), por ejemplo, si uno de los usuarios necesita permiso para administrar recursos de Azure AD, debemos asignarlo a un rol que proporcione los permisos que necesita.\nEsta asignación de roles se realiza desde la página Roles siguiendo unos sencillos pasos:\n Accedemos con nuestro usuario de administrador global a Azure Portal. En la barra superior busca y selecciona Azure Active Directory. Selecciona Usuarios. Ahora seleccionamos el usuario que obtiene la asignación de roles. Por ejemplo, Alain Charon. En la página Alain Charon - Perfil, selecciona Roles asignados. Y aquí podemos ver los roles que están asignados para este usuario. Si queremos darle roles de administrador de aplicaciones, por ejemplo, debemos seleccionar este rol y pulsamos en añadir (Add).  ##Creación masiva de usuarios en Azure Active Directory En este punto seguro que adivino si piensas que todo esto es muy bonito pero es un lento, lento, lento proceso que no puedes asumir si tenemos que ir usuario a usuario. Y tienes toda la razón.\nAzure Active Directory (Azure AD) admite operaciones de creación y eliminación de usuarios en lotes y la descarga de listas de usuarios. Solo necesitamos rellenar una plantilla .CSV que se puede descargar en el portal de Azure AD.\n##Grupos de usuarios Un grupo Azure AD permite organizar a los usuarios, facilitando la gestión de los permisos. Los grupos permiten al propietario del recurso (o al propietario de Azure AD-Directory) asignar un conjunto de permisos de acceso a todos los miembros del grupo.\nLos grupos permiten definir una política y luego añadir y eliminar usuarios específicos. De este modo, se puede conceder o denegar el acceso con un esfuerzo mínimo.\nAún mejor, Azure AD ofrece la posibilidad de definir la afiliación en función de reglas, como el departamento en el que trabaja un usuario o el cargo que ocupa.\nEn Azure AD podemos definir dos tipos diferentes de grupos:\n Grupos de seguridad. Son los grupos de seguridad más comunes y se utilizan para gestionar el acceso de los miembros y de los ordenadores a los recursos compartidos por un grupo de usuarios. Por ejemplo, puede crear un grupo de seguridad para una política de seguridad específica. De esta manera, puede dar un conjunto de permisos a todos los miembros a la vez en lugar de añadir permisos individualmente para cada miembro. Esta opción requiere un administrador de Azure AD. Grupos de Microsoft 365. Estos grupos ofrecen oportunidades de colaboración al dar a los miembros acceso a una bandeja de entrada, un calendario y archivos compartidos, SharePoint y más. Esta opción también le permite dar acceso al grupo a personas ajenas a su organización. Esta opción está disponible tanto para los usuarios como para los administradores.  ##Grupos dinámico en Azure AD En Azure Active Directory (Azure AD), también podemos usar reglas para determinar la pertenencia a grupos según las propiedades de usuario o dispositivo. La pertenencia dinámica es compatible con grupos de seguridad o grupos de Microsoft 365. Cuando se aplica una regla de pertenencia a grupos, se evalúan los atributos de usuario y dispositivo para ver si coinciden con la regla de pertenencia. Cuando cambian los atributos de un usuario o dispositivo, se procesan todos los cambios de pertenencia de las reglas de grupo dinámico de la organización.\n##Control de acceso basado en rol de Azure (RBAC) En términos de computación en la nube, el control de acceso juega un papel vital para administrar los permisos de manera efectiva. Azure RBAC es un sistema de autorización basado en Azure Resource Manager que proporciona administración de acceso específico a los recursos de Azure. La administración de acceso de los recursos en la nube es una función importantísima para cualquier organización. Azure RBAC ayuda a administrar quién tiene acceso a los recursos de Azure, qué pueden hacer con esos recursos y a qué áreas puede acceder.\n##Algunos escenarios para Azure Access Control (Azure RBAC)\n Otorgar acceso a diferentes recursos de Azure con uno o varios roles. Ejemplo: permitir que un usuario administre la aplicación de Azure y el servicio SQL de Azure. Podemos dar Contributor, Reader, owner, Manage role, Monitor reader/contributor, website contributor etc. Del mismo modo, otorgar acceso a nivel de suscripción. Ejemplo: permitir que un usuario cree solo una máquina virtual en una suscripción específica. O permita que el usuario cree el servicio de aplicaciones de Azure, así como servicios lógicos (múltiples) con rol de contribución, lectura o propietario. Dar diferentes accesos a diferentes ámbitos como grupo de administración, suscripciones, grupo de recursos o recursos. Otorgar acceso a una aplicación para acceder a los recursos también. Podemos hacer de muchas a muchas relaciones entre roles, ámbitos y usuarios o grupo (principal de seguridad).  Hay tres componentes principales que se deben comprender para el control de acceso basado en roles de Azure: entidad de seguridad (quién), rol (qué) y alcance (dónde).\nEl director de seguridad básicamente representa quién obtendrá el acceso, como usuarios, grupo, director de servicio e identidad administrada. (Quien es)\nÁmbito es el conjunto de recursos al que se aplica el acceso. Cuando se asigna un rol, es posible limitar aún más las acciones permitidas si se define un ámbito. Esto resulta útil si desea convertir a alguien en Colaborador del sitio web, pero solo para un grupo de recursos.\nEn Azure, puede especificar un ámbito en cuatro niveles: grupo de administración, suscripción, grupo de recursos o recurso. Los ámbitos se estructuran en una relación de elementos primarios y secundarios. Puede asignar roles en cualquiera de estos niveles de ámbito.\nDiagrama que muestra los niveles de ámbito de una asignación de roles.\nLas relaciones entre el principio de seguridad, la definición de rol y el alcance son una especie de muchos a muchos.\nPodemos asignar roles a un usuario o grupo en un cierto alcance para el control de acceso y nuevamente podemos revocarlos eliminando una asignación de roles. Podemos asignar el mismo rol a múltiples usuarios, grupos o identidad administrada en recursos iguales o diferentes (alcance). Podemos asignar roles utilizando Azure Portal, Azure SDK, Azure CLI, Azure PowerShell o API REST. La forma más fácil de manejar el control de acceso basado en roles es mediante el Portal de Azure.\nEncontraremos una pestaña común en todos los recursos de Azure que es Control de acceso (IAM) como se muestra, y a partir de aquí podemos realizar nuestra asignación a medida.\n","date":"Nov 9, 2021","img":"https://manuss20.com/images/posts/Azure-ARM.jpg","permalink":"https://manuss20.com/2021/11/09/azure-ad-managed-identity/","series":null,"tags":["Azure","Azure AD","Managed Identity","Identity","Users","Groups","Privileges","Cloud"],"title":"Azure AD Managed Identity"},{"categories":["Azure","Azure-Arc","Arc","Hybrid","IoT","Kubernetes","AKS","Jumpstart"],"content":" This scenario was published in Azure Arc JumpStart\n Deploy AKS cluster on Azure IoT Edge and connect it to Azure Arc using Terraform This scenario allows us to see how Azure IoT Edge and Azure Arc services complement each other in an easy and simple way, providing mechanisms for AKS cluster operators to configure the fundamental components of an AKS cluster and apply policies by monitoring its supervision, through Azure Arc. Furthermore, from Azure IoT Edge, application operators can remotely deploy and manage workloads at scale with convenient ingest from the cloud and in a bi-directional way.\n Note: Azure Kubernetes Service is now in preview on Azure IoT Edge. You can find more details about this service in the IoT Edge\u0026rsquo;s support for Kubernetes document\n The following README will guide you on how to use the provided Terraform plan to deploy an Azure Kubernetes Service (AKS) cluster and connect it as an Azure Arc-enabled Kubernetes resource.\nPrerequisites   Clone the Azure Arc Jumpstart repository\n1git clone https://github.com/microsoft/azure_arc.git   Install or update Azure CLI to version 2.25.0 and above. Use the below command to check your current installed version.\n1az --version   Install Terraform \u0026gt;=0.15\n  Create Azure service principal (SP)\nTo be able to complete the scenario and its related automation, Azure service principal assigned with the “Contributor” role is required. To create it, login to your Azure account run the below command (this can also be done in Azure Cloud Shell).\n1az login 2az ad sp create-for-rbac -n \u0026#34;\u0026lt;Unique SP Name\u0026gt;\u0026#34; --role contributor For example:\n1az ad sp create-for-rbac -n \u0026#34;http://AzureArcK8s\u0026#34; --role contributor Output should look like this:\n1{ 2\u0026#34;appId\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34;, 3\u0026#34;displayName\u0026#34;: \u0026#34;AzureArcK8s\u0026#34;, 4\u0026#34;name\u0026#34;: \u0026#34;http://AzureArcK8s\u0026#34;, 5\u0026#34;password\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34;, 6\u0026#34;tenant\u0026#34;: \u0026#34;XXXXXXXXXXXXXXXXXXXXXXXXXXXX\u0026#34; 7}  Note: The Jumpstart scenarios are designed with as much ease of use in-mind and adhering to security-related best practices whenever possible. It is optional but highly recommended to scope the service principal to a specific Azure subscription and resource group as well considering using a less privileged service principal account\n   Enable subscription with the two resource providers for Azure Arc-enabled Kubernetes. Registration is an asynchronous process, and registration may take approximately 10 minutes.\n1az provider register --namespace Microsoft.Kubernetes 2az provider register --namespace Microsoft.KubernetesConfiguration 3az extension add --name connectedk8s 4az extension add --name k8sconfiguration You can monitor the registration process with the following commands:\n1az provider show -n Microsoft.Kubernetes -o table 2az provider show -n Microsoft.KubernetesConfiguration -o table   Automation Flow For you to get familiar with the automation and deployment flow, below is an explanation.\n  First bash script [edge_azure_vm.sh]((https://raw.githubusercontent.com/microsoft/azure_arc/main/azure_arc_k8s_jumpstart/aks_iot_edge/terraform/scripts/edge/edge_azure_vm.sh) - Used specifically for provisioning the necessary components in the VM to be able to deploy our \u0026ldquo;simulated\u0026rdquo; edge device:\n Download Install the required tools moby-engine Download \u0026amp; install the Azure aziot-edge Creation of a new configuration file for aziot-edge (/etc/aziot/config.toml)    Second bash script az_k8sconfig_helm_aks.sh Allow us to deploy our IoT Edge solution for AKS, configure our and associate our AKS cluster with Azure Arc, for this:\n Log in to Azure with Service Principal \u0026amp; Getting AKS credentials (kubeconfig) Associate our AKS with Azure Arc Create Namespace iotedge in AKS Generate a secret that contains the connection string of our edge device. Create Cluster-level GitOps-Config for deploying IoT Edge workload    Deployment Before running the Terraform automation, you need to export the environment variables that will be used by the plan to customize your environment.\nIn addition, validate that the AKS Kubernetes version is available in your region using the below Azure CLI command.\n1az aks get-versions -l \u0026#34;\u0026lt;Your Azure Region\u0026gt;\u0026#34; In case the AKS service is not available in your region, you can change the AKS Kubernetes version in the variables.tf file by searching for kubernetes_version.\n  Export the environment variables needed for the Terraform plan.\n1export TF_VAR_client_id=\u0026lt;Your Azure service principal App ID\u0026gt; 2export TF_VAR_client_secret=\u0026lt;Your Azure service principal App Password\u0026gt;  Note: If you are running in a PowerShell environment, to set the Terraform environment variables, use the Set-Item -Path env: prefix (see example below)\n 1Set-Item -Path env:TF_VAR_client_id   Run the terraform init command which will download the Terraform AzureRM provider.\n  Run the ```terraform apply -auto-approve`` command and wait for the plan to finish.\nOnce the Terraform deployment is completed, a new Resource Group and all services (Vnet, Subnets, VMs, IoT Hub, EventHub, AKS Cluster) are created.\n  In this scenario we will use a VM to \u0026ldquo;simulate\u0026rdquo; an IoT Edge device. To do this, we must register a new Edge device in our IoT Hub that we will later configure.\n  In order to keep your local environment clean and untouched, we will use Azure Cloud Shell (located in the top-right corner in the Azure portal) to run the next commands:\n  Create IoT Edge Device\n  1az iot hub device-identity create --device-id \u0026#34;EdgeDeviceSim\u0026#34; --edge-enabled --hub-name k8sedgejumpstart   We will obtain the connection string of the new IoT Edge device to be able to make the link   1az iot hub device-identity connection-string show --device-id \u0026#34;EdgeDeviceSim\u0026#34; --hub-name k8sedgejumpstart  Next, log into the deployment VM using your SSH credentials and edit the /etc/aziot/config.toml by replacing the connection string using the one we obtained in the previous step.  1# Manual provisioning with connection string 2[provisioning] 3source = \u0026#34;manual\u0026#34; 4connection_string = \u0026#34;\u0026lt;ADD DEVICE CONNECTION STRING HERE\u0026gt;\u0026#34;  In order to synchronize the configuration of the device that we have paired we must execute the following command:  1sudo iotedge config apply  Once completed the above steps, return to the Azure Cloud Shell where we will assign to our new device a module to simulate a temperature sensor. For this we will upload the file through the Azure Cloud Shell interface:   Once the file is uploaded, execute the following command:   Note: You can see an example of the deployment.json file that we use.\n 1az iot edge set-modules --hub-name k8sedgejumpstart --device-id \u0026#34;EdgeDeviceSim\u0026#34; --content ./deployment.json  From the Azure portal, select the IoT Hub instance under K8sEdgeJumpStart. By selecting our IoT Edge device, we can see all the information about the modules it is running and If everything has been successful we will see that the \u0026ldquo;SimulatedTemperatureSensor\u0026rdquo; module is running correctly.   We can also check from the virtual machine itself the modules that are running at that moment, using the following command:  1sudo iotedge list  Now download the values.yaml file for IoT Edge Helm chart and replace the deviceConnectionString placeholder at the end of the file with the connection string you noted earlier.  1# Manual provisioning configuration using a connection string2provisioning:3source:\u0026#34;manual\u0026#34;4deviceConnectionString:\u0026#34;\u0026lt;ADD DEVICE CONNECTION STRING HERE\u0026gt;\u0026#34;5dynamicReprovisioning:false Edit the environment variables section in the included in the az_k8sconfig_helm_aks.sh shell script. As we did in the previous steps, upload the files to our Azure Cloud Shell.   Once the script run has finished, the AKS cluster will be projected as a new Azure Arc-enabled Kubernetes resource. We will proceed to connect to our AKS cluster and in a couple of minutes you should see the workload modules defined in the edge deployment running as pods along with edgeagent and iotedged. We can use the following commands to check it:  1kubectl get pods -n iotedge 2kubectl logs -n iotedge \u0026lt;replace-with-iot-edge-pod-name\u0026gt; simulatedtemperaturesensor Delete the deployment The most straightforward way is to delete the Azure Arc-enabled Kubernetes resource via the Azure Portal, just select the Resource Group and delete it.\nIf you want to nuke the entire environment, delete both the AKS and the AKS resources resource groups or run the terraform destroy -auto-approve command.\nSaludos!\n","date":"Jun 6, 2021","img":"https://manuss20.com/images/posts/azure_arc_jumpstart_logo.png","permalink":"https://manuss20.com/2021/06/06/azure-iot-edge-integration-with-aks-as-an-azure-arc-connected-cluster/","series":null,"tags":["Azure","Azure-Arc","Arc","Hybrid","IoT","Kubernetes","AKS","Jumpstart"],"title":"Azure IoT Edge Integration With AKS as an Azure Arc Connected Cluster"},{"categories":["Azure","Azure-Storage-Account","sta","Terraform"],"content":"En este post quiero explicaros como podemos desplegar Infraestructura como código a través de Terraform, pero en vez de usar la sintaxis nativa usaremos la alternativa que es compatible con JSON.\n¿Esto como sucede? Terraform también soporta una sintaxis alternativa que es compatible con JSON. Esta sintaxis es útil cuando se generan partes de una configuración mediante programación, ya que se pueden utilizar las bibliotecas JSON existentes para preparar los archivos de configuración generados. La sintaxis JSON se define en términos de la sintaxis nativa. Todo lo que se puede expresar en sintaxis nativa también se puede expresar en sintaxis JSON, pero algunas construcciones son más complejas de representar en JSON debido a las limitaciones de la gramática JSON. Terraform espera la sintaxis nativa para los archivos nombrados con un sufijo .tf, y la sintaxis JSON para los archivos nombrados con un sufijo .tf.json.\n¿Qué son los Azure Storage Accounts? Azure Storage Account contiene todos los objetos de datos de Azure Storage, incluidos blobs, recursos compartidos de archivos, colas, tablas y discos. La cuenta de almacenamiento (Storage Accounts) proporciona un espacio de nombres único para los datos de Azure Storage que es accesible desde cualquier lugar del mundo mediante HTTP o HTTPS. Los datos de la cuenta de almacenamiento son duraderos y altamente disponibles, seguros y escalables a gran escala. Una vez que tenemos esto, vamos a desplegar un Storage Account a través de Terraform usando la sintaxis alternativa:\nmain.tf.json\n1{ 2\t\u0026#34;data\u0026#34;: { 3\t\u0026#34;azurerm_subscription\u0026#34;: { 4\t\u0026#34;current\u0026#34;: {} 5\t}, 6\t\u0026#34;azurerm_client_config\u0026#34;: { 7\t\u0026#34;current\u0026#34;: {} 8\t}, 9\t\u0026#34;azuread_domains\u0026#34;: { 10\t\u0026#34;aad_domains\u0026#34;: { 11\t\u0026#34;only_default\u0026#34;: \u0026#34;true\u0026#34; 12\t} 13\t} 14\t}, 15\t\u0026#34;resource\u0026#34;: { 16\t\u0026#34;azurerm_resource_group\u0026#34;: { 17\t\u0026#34;rg\u0026#34;: { 18\t\u0026#34;name\u0026#34;: \u0026#34;${var.resource_group_name}\u0026#34;, 19\t\u0026#34;location\u0026#34;: \u0026#34;${var.location}\u0026#34;, 20\t\u0026#34;tags\u0026#34;: \u0026#34;${var.tags}\u0026#34; 21\t} 22\t} 23\t} 24} providers.tf.json\n1{ 2\t\u0026#34;data\u0026#34;: { 3\t\u0026#34;azurerm_subscription\u0026#34;: { 4\t\u0026#34;current\u0026#34;: {} 5\t}, 6\t\u0026#34;azurerm_client_config\u0026#34;: { 7\t\u0026#34;current\u0026#34;: {} 8\t}, 9\t\u0026#34;azuread_domains\u0026#34;: { 10\t\u0026#34;aad_domains\u0026#34;: { 11\t\u0026#34;only_default\u0026#34;: \u0026#34;true\u0026#34; 12\t} 13\t} 14\t}, 15\t\u0026#34;resource\u0026#34;: { 16\t\u0026#34;azurerm_resource_group\u0026#34;: { 17\t\u0026#34;rg\u0026#34;: { 18\t\u0026#34;name\u0026#34;: \u0026#34;${var.resource_group_name}\u0026#34;, 19\t\u0026#34;location\u0026#34;: \u0026#34;${var.location}\u0026#34;, 20\t\u0026#34;tags\u0026#34;: \u0026#34;${var.tags}\u0026#34; 21\t} 22\t} 23\t} 24} sta.tf.json\n1{ 2 \u0026#34;resource\u0026#34;: { 3 \u0026#34;azurerm_storage_account\u0026#34;: { 4 \u0026#34;sta\u0026#34;: { 5 \u0026#34;name\u0026#34;: \u0026#34;${var.name-sta}\u0026#34;, 6 \u0026#34;resource_group_name\u0026#34;: \u0026#34;${azurerm_resource_group.rg.name}\u0026#34;, 7 \u0026#34;location\u0026#34;: \u0026#34;${var.location}\u0026#34;, 8 \u0026#34;account_tier\u0026#34;: \u0026#34;${var.sku-sta}\u0026#34;, 9 \u0026#34;account_replication_type\u0026#34;: \u0026#34;${var.replication-type-sta}\u0026#34;, 10 \u0026#34;enable_https_traffic_only\u0026#34;: \u0026#34;${var.http-traffic-only-sta}\u0026#34;, 11 \u0026#34;tags\u0026#34;: \u0026#34;${var.tags}\u0026#34; 12 } 13 }, 14 \u0026#34;azurerm_storage_container\u0026#34;: { 15 \u0026#34;sta_container\u0026#34;: { 16 \u0026#34;name\u0026#34;: \u0026#34;${var.name-sta-container}\u0026#34;, 17 \u0026#34;storage_account_name\u0026#34;: \u0026#34;${azurerm_storage_account.sta.name}\u0026#34;, 18 \u0026#34;container_access_type\u0026#34;: \u0026#34;${var.access-type-sta-container}\u0026#34; 19 } 20 } 21 } 22} variable.tf\n1#Commons 2variable \u0026#34;location\u0026#34; { 3 description = \u0026#34;(Required) Location of the all services to be created\u0026#34; 4 default=\u0026#34;westeurope\u0026#34; 5} 6 7variable \u0026#34;resource_group_name\u0026#34; { 8 description = \u0026#34;(Required) Resource group name of the all services to be created\u0026#34; 9 default= \u0026#34;TestTF\u0026#34; 10} 11 12variable \u0026#34;tags\u0026#34; { 13 description = \u0026#34;(Required) Tags to be applied to the all services to be created\u0026#34; 14 default = { Project = \u0026#34;jumpstart_azure_tf\u0026#34; } 15} 16 17# Storage Account 18variable \u0026#34;name-sta\u0026#34; { 19 description = \u0026#34;(Required) Name of Storage Account\u0026#34; 20 default=\u0026#34;statestingaz\u0026#34; 21} 22 23variable \u0026#34;sku-sta\u0026#34; { 24 description = \u0026#34;(Required) Sku Storage Account\u0026#34; 25 default=\u0026#34;Standard\u0026#34; 26} 27 28variable \u0026#34;replication-type-sta\u0026#34; { 29 description = \u0026#34;(Required) Replication type on Storage Account\u0026#34; 30 default=\u0026#34;LRS\u0026#34; 31} 32 33variable \u0026#34;http-traffic-only-sta\u0026#34; { 34 description = \u0026#34;(Required) http-traffic-only-sta Storage Account\u0026#34; 35 default=true 36} 37 38variable \u0026#34;name-sta-container\u0026#34; { 39 description = \u0026#34;(Required) Name of Storage Account Container\u0026#34; 40 default=\u0026#34;tfstate\u0026#34; 41} 42 43variable \u0026#34;access-type-sta-container\u0026#34; { 44 description = \u0026#34;(Required) Access Type of Storage Account Container\u0026#34; 45 default=\u0026#34;private\u0026#34; 46} De esta forma tendremos nuestro proyecto completamente automatizado.\nSaludos!\n","date":"Jun 6, 2021","img":"https://manuss20.com/images/posts/Terraform.png","permalink":"https://manuss20.com/2021/06/06/deploy-azure-storage-account-con-terraform-json/","series":null,"tags":["Azure","Azure-Storage-Account","sta","Terraform"],"title":"Deploy Azure Storage Account Con Terraform JSON"},{"categories":["Azure","DAPR","Microservices","Microservicios","Micosoft"],"content":"En los últimos años, las arquitecturas de microservicios se han convertido en una opción popular entre los desarrolladores de la nube debido a sus ventajas, como la escabilidad, el acoplamiento de servicio flexible y las implementaciones independientes.\nCada servicio tiene que implementar una clase de almacén de datos como relacional, clave / valor, NoSQL y base de datos gráfica alineada con la funcionalidad. Los microservicios deben tener un mecanismo robusto de descubrimiento de servicios para la conectividad dinámica. Deben estar acoplados libremente para lograr autonomía y escalamiento independiente.\nLos microservicios son políglotas donde cada servicio se implementa en el lenguaje, el marco y el tiempo de ejecución más apropiados.\nAunque la adopción de contenedores y motores de orquestación como Kubernetes abordan los desafíos en el empaquetado, la implementación y el escalado, el proceso de desarrollo sigue siendo complejo.\nA fines del año pasado, Microsoft anunció un nuevo enfoque para desarrollar aplicaciones modernas basadas en el Tiempo de ejecución de aplicaciones distribuidas (Dapr), que es un tiempo de ejecución agnóstico de plataforma y lenguaje para microservicios y aplicaciones nativas de la nube.\nHay muchas consideraciones al diseñar aplicaciones de microservicios. Dapr proporciona las mejores prácticas para capacidades comunes al crear aplicaciones de microservicio que los desarrolladores pueden usar de manera estándar e implementar en cualquier entorno. Lo hace al proporcionar bloques de construcción del sistema distribuido.\nPara hacer que el uso de Dapr sea más natural para diferentes idiomas, también incluye SDK específicos de idioma para Go, Java, JavaScript, .NET y Python. Estos SDK exponen la funcionalidad en los bloques de construcción Dapr, como guardar el estado, publicar un evento o crear un actor, a través de una API de idioma escrita en lugar de llamar a la API http / gRPC. Esto le permite escribir una combinación de funciones y actores sin estado y con estado, todo en el idioma que elija. Y debido a que estos SDK comparten el tiempo de ejecución de Dapr, obtienes soporte de actores y funciones en varios idiomas.\nAdemás, Dapr se puede integrar con cualquier marco de desarrollador. Por ejemplo, en Dapr .NET SDK puede encontrar la integración de ASP.NET Core, que trae controladores de enrutamiento con estado que responden a eventos pub / sub de otros servicios. Y en Dapr Java SDK puede encontrar la integración de Spring Boot.\nSaludos!\n","date":"Mar 14, 2020","img":"https://manuss20.com/images/posts/dapr-scaled.jpg","permalink":"https://manuss20.com/2020/03/14/dapr/","series":null,"tags":["Azure","DAPR","Microservices","Microservicios","Micosoft"],"title":"DAPR"},{"categories":["Azure","Container","Docker","DevOps","KeyVault","Cloud","Azure ARM"],"content":"En este post quiero explicaros como podemos desplegar nuestro entorno de desarrollo de una forma fácil y segura. Para ello vamos a usar docker para poder crear un contenedor con nuestra aplicación “API netcore 3.1” que conecta contra nuestra base de datos Azure SQL de forma segura sin tener que exponer nuestras credenciales.\n Docker es una plataforma para desarrolladores y sysadmins (utlizando la filosofía DevOps) que nos permite desarrollar, desplegar y ejecutar aplicaciones en contenedores de una forma fácil y sencilla.\n Para ello vamos a empezar creando nuestra API en .NET Core 3.1, pare ello usaremos el siguiente comando:\n1dotnet new API -n Azuretraining Esto nos creara la estructura de nuestro proyecto con el nombre que le hemos asignado «Azuretraining» tal y como podemos observar en la siguiente imagen:\nAhora vamos a agregar el nuget para poder trabajar con SQL en nuestro proyecto:\n1dotnet add package Microsoft.EntityFrameworkCore.SqlServer 2dotnet add package Microsoft.EntityFrameworkCore.InMemory Incorporaremos una nueva clase al modelo, para este ejemplo definiremos un modelo de ejemplo llamado «Courses» dentro de nuestra carpeta Models:\n1namespace Azuretraining.Models 2{ 3 public class Course 4 { 5 public long Id { get; set; } 6 public string Name { get; set; } 7 public string Description { get; set; } 8 public DateTime StartDate { get; set; } 9 public DateTime EndDate { get; set; } 10 public int Capacity {get; set;} 11 public double Qualification {get; set;} 12 public string Modality {get; set;} 13 public string Category {get; set;} 14 public bool IsComplete { get; set; } 15 } 16} Una vez tenemos nuestro modelo incorporaremos el contexto de base de datos, será la clase principal que coordina la funcionalidad de Entity Framework para un modelo de datos. Para ello agregaremos a nuestra carpeta Models un nuevo fichero llamado «CourseContext.cs» con el siguiente formato:\n1using Microsoft.EntityFrameworkCore; 2 3namespace Azuretraining.Models 4{ 5 public class CourseContext : DbContext 6 { 7 public CourseContext(DbContextOptions\u0026lt;CourseContext\u0026gt; options) 8 : base(options) 9 { 10 } 11 12 public DbSet\u0026lt;Course\u0026gt; Courses { get; set; } 13 } 14} Ahora deberemos de modificar nuestro fichero «Startup.cs» agregando las referencias necesarias para usar los servicios:\n1using Microsoft.EntityFrameworkCore; 2using Azuretraining.Models; 3Y modificaremos nuestra función «ConfigureServices» para que tenga el siguiente aspecto: 4 5public void ConfigureServices(IServiceCollection services) 6{ 7 services.AddDbContext\u0026lt;CourseContext\u0026gt;(opt =\u0026gt; 8 opt.UseInMemoryDatabase(\u0026#34;CourseList\u0026#34;)); 9 services.AddControllers(); 10} Esto nos proporcionará poder usar una BD en memoria para realizar unas primeras pruebas antes de atacar a nuestra BD en la nube. Una vez lo tenemos todo preparado vamos a agregar los Nugets necesarios para poder hacer el scaffolding y generar de forma automática nuestro Controller con las siguientes instrucciones:\n1dotnet add package Microsoft.VisualStudio.Web.CodeGeneration.Design 2dotnet add package Microsoft.EntityFrameworkCore.Design 3dotnet tool install --global dotnet-aspnet-codegenerator 4dotnet aspnet-codegenerator controller -name CoursesController -async -api -m Course -dc CourseContext -outDir Controllers Veremos que en la carpeta Controller nos ha generado el archivo «CoursesController.cs» donde tendremos definida todas las acciones de nuestra API. Ahora ejecutaremos nuestra aplicación y en un explorador introducimos la siguiente URL: https://localhost:5001/api/courses.\nAhora vamos a modificar nuestra aplicación para que conecte directamente con nuestra BD de Azure SQL. Para ello previamente deberemos haber creado nuestra instancia, podemos usar Azure CLI para poder hacerlo como se muestra en el siguiente ejemplo:\n1az sql server create --subscription \u0026#34;NOMBRE DE LA SUSCRIPCIÓN\u0026#34; --name trainginappDB --resource-group TrainingApp --location \u0026#34;West Europe\u0026#34; --admin-user \u0026#34;NOMBRE DE USUARIO\u0026#34; --admin-password \u0026#34;PASSWORD\u0026#34; 2 3az sql server firewall-rule create --subscription \u0026#34;NOMBRE DE LA SUSCRIPCIÓN\u0026#34; --resource-group TrainingApp --server trainginappdb --name AllowAllIps --start-ip-address 0.0.0.0 --end-ip-address 0.0.0.0 4 5az sql db create --subscription \u0026#34;NOMBRE DE LA SUSCRIPCIÓN\u0026#34; --resource-group TrainingApp --server trainginappdb --name TrainingApp --service-objective S0 Una vez tenemos creada nuestra instancia de BD en Azure SQL, vamos a preparar nuestra solución para «dockerizar», para ello generaremos un fichero .Dockerfile con el siguiente contenido:\n1# https://hub.docker.com/_/microsoft-dotnet-core 2 FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build 3 WORKDIR /app 4 5# copy csproj and restore as distinct layers 6 COPY *.csproj ./ 7 RUN dotnet restore 8 9# copy everything else and build app 10 COPY . ./ 11 RUN dotnet publish -c release -o out --no-restore 12 13# final stage/image 14 FROM mcr.microsoft.com/dotnet/core/aspnet:3.1 15 WORKDIR /app 16 COPY --from=build /app/out . 17 ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;azuretraining.dll\u0026#34;] 18Y un fichero «.dockerignore» en nuestra solución con el siguiente contenido: 19 20# directories 21**/bin/ 22**/obj/ 23**/out/ 24 25# files 26Dockerfile* 27**/*.md Para nuestra cadena de conexión usaremos Azure KeyVault para poder proteger nuestro «secretos», para ello iremos al portal de Azure y crearemos un nuevo Azure KeyVault. Una vez creado vamos a Secrets -\u0026gt; Generate/Import como se puede apreciar en la siguiente captura:\nEn la siguiente pantalla deberemos de indicar que es una entrada manual, le damos un nombre a nuestro secreto, en este caso «ConnectionStrings–TrainingConnection» esto se debe a que en nuestro fichero «appsettings.json» tenemos la definición de nuestro ConnectionStrings de la siguiente forma y para que el KeyVault pueda insertar el valor en tiempo de ejecución debemos de separarlos con «–» el nombre concatenando la relación padre-hijo:\nAhora añadimos la cadena de conexión hacia nuestro Azure SQL que nos facilita cuando creamos el servicio, como se puede apreciar en la siguiente captura:\nUna vez que ya tenemos nuestro KeyVault para poder proteger nuestros «secretos» vamos a modificar nuestro proyecto para poder usarlo, para ello necesitaremos añadir los siguiente Nugets:\n1dotnet add package Microsoft.Azure.KeyVault 2dotnet add package Microsoft.Azure.Services.AppAuthentication 3dotnet add package Microsoft.Extensions.Configuration.AzureKeyVault 4Modificaremos nuestro archivo «Program.cs» añadiremos los imports necesarios: 1using Microsoft.Azure.KeyVault; 2using Microsoft.Azure.Services.AppAuthentication; 3using Microsoft.Extensions.Configuration; 4using Microsoft.Extensions.Configuration.AzureKeyVault; Sustituiremos el método IHostBuilder para poder obtener la información de nuestro KeyVault y asignarlo el siguiente formato:\n1public static IHostBuilder CreateHostBuilder(string[] args) =\u0026gt; 2 Host.CreateDefaultBuilder(args) 3 .ConfigureAppConfiguration((ctx, builder) =\u0026gt; 4 { 5 var keyVaultEndpoint = GetKeyVaultEndpoint(); 6 if (!string.IsNullOrEmpty(keyVaultEndpoint)) 7 { 8 var azureServiceTokenProvider = new AzureServiceTokenProvider(); 9 var keyVaultClient = new KeyVaultClient( 10 new KeyVaultClient.AuthenticationCallback( 11 azureServiceTokenProvider.KeyVaultTokenCallback)); 12 builder.AddAzureKeyVault( 13 keyVaultEndpoint, keyVaultClient, new DefaultKeyVaultSecretManager()); 14 } 15 }) 16 .ConfigureWebHostDefaults(webBuilder =\u0026gt; 17 { 18 webBuilder.UseStartup\u0026lt;Startup\u0026gt;(); 19 }); 20static string GetKeyVaultEndpoint() =\u0026gt; Environment.GetEnvironmentVariable(\u0026#34;KEYVAULT_ENDPOINT\u0026#34;); Ahora agregaremos en el environment (todo esto lo hacemos para que la acción se realice en tiempo de ejecución), para ello nos fijaremos que en la última linea de nuestro «Program.cs» indicábamos obtener de la variable «KEYVAULT_ENDPOINT» en ella declararemos la URL de nuestro Azure KeyVaul, esta información la deberemos de añadirla a nuestro fichero «launch.json» con el siguiente formato:\n1{ 2 // Use IntelliSense to find out which attributes exist for C# debugging 3 // Use hover for the description of the existing attributes 4 // For further information visit https://github.com/OmniSharp/omnisharp-vscode/blob/master/debugger-launchjson.md 5 \u0026#34;version\u0026#34;: \u0026#34;0.2.0\u0026#34;, 6 \u0026#34;configurations\u0026#34;: [ 7 { 8 \u0026#34;name\u0026#34;: \u0026#34;.NET Core Launch (web)\u0026#34;, 9 \u0026#34;type\u0026#34;: \u0026#34;coreclr\u0026#34;, 10 \u0026#34;request\u0026#34;: \u0026#34;launch\u0026#34;, 11 \u0026#34;preLaunchTask\u0026#34;: \u0026#34;build\u0026#34;, 12 // If you have changed target frameworks, make sure to update the program path. 13 \u0026#34;program\u0026#34;: \u0026#34;${workspaceFolder}/bin/Debug/netcoreapp3.1/trainingapp.courses.dll\u0026#34;, 14 \u0026#34;args\u0026#34;: [], 15 \u0026#34;cwd\u0026#34;: \u0026#34;${workspaceFolder}\u0026#34;, 16 \u0026#34;stopAtEntry\u0026#34;: false, 17 // Enable launching a web browser when ASP.NET Core starts. For more information: https://aka.ms/VSCode-CS-LaunchJson-WebBrowser 18 \u0026#34;serverReadyAction\u0026#34;: { 19 \u0026#34;action\u0026#34;: \u0026#34;openExternally\u0026#34;, 20 \u0026#34;pattern\u0026#34;: \u0026#34;^\\\\s*Now listening on:\\\\s+(https?://\\\\S+)\u0026#34; 21 }, 22 \u0026#34;env\u0026#34;: { 23 \u0026#34;ASPNETCORE_ENVIRONMENT\u0026#34;: \u0026#34;Development\u0026#34;, 24 \u0026#34;KEYVAULT_ENDPOINT\u0026#34;: \u0026#34;https://NOMBREDENUESTROKEYVAULT.vault.azure.net/\u0026#34; 25 }, 26 \u0026#34;sourceFileMap\u0026#34;: { 27 \u0026#34;/Views\u0026#34;: \u0026#34;${workspaceFolder}/Views\u0026#34; 28 } 29 }, 30 { 31 \u0026#34;name\u0026#34;: \u0026#34;.NET Core Attach\u0026#34;, 32 \u0026#34;type\u0026#34;: \u0026#34;coreclr\u0026#34;, 33 \u0026#34;request\u0026#34;: \u0026#34;attach\u0026#34;, 34 \u0026#34;processId\u0026#34;: \u0026#34;${command:pickProcess}\u0026#34; 35 } 36 ] 37} Por ultimo sustituiremos en nuestro fichero «Startup.cs» la conexión de la BD en memoria por la conexión hacia nuestro Azure SQL con la configuración que hemos preparado en los pasos anteriores, y quedará de la siguiente forma:\nEsta primer servicio es la conexión que realizamos para conectar con nuestra «Cadena de conexión» securizada:\n1services.AddDbContext(options =\u0026gt; 2 options.UseSqlServer(Configuration.GetConnectionString(\u0026#34;TrainingConnection\u0026#34;))); Este segundo servicio nos permitirá crear las tabla y estructura iniciales en caso de que no lo tengamos:\n1services.BuildServiceProvider().GetService().Database.Migrate(); Ahora lanzamos nuestra aplicación y vemos que nos ha funcionado correctamente:\nATENCIÓN: Como hemos podido ver hasta aquí lo único que hemos echo es indicar la url de nuestro Azure KeyVault para poder recuperar la información de la cadena de conexión, pero el «truco» es que sino estamos logados en nuestro azure CLI en local no podremos usarlo y nos devolverá el siguiente error:\n1Startup.cs(34,13): warning ASP0000: Calling \u0026#39;BuildServiceProvider\u0026#39; from application code results in an additional copy of singleton services being created. Consider alternatives such as dependency injecting services as parameters to \u0026#39;Configure\u0026#39;. [/Users/msanchez/Projects/Azuretraining/Azuretraining.csproj] 2Unhandled exception. System.ArgumentNullException: Value cannot be null. (Parameter \u0026#39;connectionString\u0026#39;) 3at Microsoft.EntityFrameworkCore.Utilities.Check.NotEmpty(String value, String parameterName) 4at Microsoft.EntityFrameworkCore.SqlServerDbContextOptionsExtensions.UseSqlServer(DbContextOptionsBuilder optionsBuilder, String connectionString, Action1 sqlServerOptionsAction) at Azuretraining.Startup.\u0026lt;ConfigureServices\u0026gt;b__4_0(DbContextOptionsBuilder options) in /Users/msanchez/Projects/Azuretraining/Startup.cs:line 32 at Microsoft.Extensions.DependencyInjection.EntityFrameworkServiceCollectionExtensions.\u0026lt;\u0026gt;c__DisplayClass1_02.b__0(IServiceProvider p, DbContextOptionsBuilder b) 5at Microsoft.Extensions.DependencyInjection.EntityFrameworkServiceCollectionExtensions.CreateDbContextOptions[TContext](IServiceProvider applicationServiceProvider, Action2 optionsAction) at Microsoft.Extensions.DependencyInjection.EntityFrameworkServiceCollectionExtensions.\u0026lt;\u0026gt;c__DisplayClass10_01.b__0(IServiceProvider p) 6at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitFactory(FactoryCallSite factoryCallSite, RuntimeResolverContext context) 7at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitCache(ServiceCallSite callSite, RuntimeResolverContext context, ServiceProviderEngineScope serviceProviderEngine, RuntimeResolverLock lockType) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitScopeCache(ServiceCallSite singletonCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor2.VisitCallSite(ServiceCallSite callSite, TArgument argument) 8at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitConstructor(ConstructorCallSite constructorCallSite, RuntimeResolverContext context) 9at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor2.VisitCallSiteMain(ServiceCallSite callSite, TArgument argument) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitCache(ServiceCallSite callSite, RuntimeResolverContext context, ServiceProviderEngineScope serviceProviderEngine, RuntimeResolverLock lockType) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.VisitScopeCache(ServiceCallSite singletonCallSite, RuntimeResolverContext context) at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteVisitor2.VisitCallSite(ServiceCallSite callSite, TArgument argument) 10at Microsoft.Extensions.DependencyInjection.ServiceLookup.CallSiteRuntimeResolver.Resolve(ServiceCallSite callSite, ServiceProviderEngineScope scope) 11at Microsoft.Extensions.DependencyInjection.ServiceLookup.DynamicServiceProviderEngine.\u0026lt;\u0026gt;c__DisplayClass1_0.b__0(ServiceProviderEngineScope scope) 12at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.GetService(Type serviceType, ServiceProviderEngineScope serviceProviderEngineScope) 13at Microsoft.Extensions.DependencyInjection.ServiceLookup.ServiceProviderEngine.GetService(Type serviceType) 14at Microsoft.Extensions.DependencyInjection.ServiceProvider.GetService(Type serviceType) 15at Microsoft.Extensions.DependencyInjection.ServiceProviderServiceExtensions.GetService[T](IServiceProvider provider) 16at Azuretraining.Startup.ConfigureServices(IServiceCollection services) in /Users/msanchez/Projects/Azuretraining/Startup.cs:line 34 17at System.RuntimeMethodHandle.InvokeMethod(Object target, Object[] arguments, Signature sig, Boolean constructor, Boolean wrapExceptions) 18at System.Reflection.RuntimeMethodInfo.Invoke(Object obj, BindingFlags invokeAttr, Binder binder, Object[] parameters, CultureInfo culture) 19at Microsoft.AspNetCore.Hosting.ConfigureServicesBuilder.InvokeCore(Object instance, IServiceCollection services) 20at Microsoft.AspNetCore.Hosting.ConfigureServicesBuilder.\u0026lt;\u0026gt;c__DisplayClass9_0.g__Startup|0(IServiceCollection serviceCollection) 21at Microsoft.AspNetCore.Hosting.ConfigureServicesBuilder.Invoke(Object instance, IServiceCollection services) 22at Microsoft.AspNetCore.Hosting.ConfigureServicesBuilder.\u0026lt;\u0026gt;c__DisplayClass8_0.b__0(IServiceCollection services) 23at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder.UseStartup(Type startupType, HostBuilderContext context, IServiceCollection services) 24at Microsoft.AspNetCore.Hosting.GenericWebHostBuilder.\u0026lt;\u0026gt;c__DisplayClass12_0.b__0(HostBuilderContext context, IServiceCollection services) 25at Microsoft.Extensions.Hosting.HostBuilder.CreateServiceProvider() 26at Microsoft.Extensions.Hosting.HostBuilder.Build() 27at Azuretraining.Program.Main(String[] args) in /Users/msanchez/Projects/Azuretraining/Program.cs:line 19 28 Este error nos dará si intentamos ejecutar nuestro contenedor de docker, para solventarlo deberemos de aplicar un «work around» que nos permita poder trabajar sin problemas y a la vez que subimos el código a cualquier repositorio de código no tengamos que mostrar nuestra cadenas de conexión o información sensible. Para ello lo que vamos a hacer es añadir un nuevo fichero llamado docker-compose.yml con la siguiente composición:\n1version: \u0026#34;3.7\u0026#34; 2 3networks: 4 azuretraining.services.network: 5 driver: bridge 6 7services: 8 azuretraining.services.courses: 9 container_name: Azuretraining.Services 10 build: 11 context: ../ 12 dockerfile: ./Azuretraining.Dockerfile 13 ports: 14 - \u0026#34;8001:80\u0026#34; 15 networks: 16 - azuretraining.services.network 17 volumes: 18 - ~/.azure:/root/.azure 19 environment: 20 - KEYVAULT_ENDPOINT=https://NOMBREDENUESTROKEYVAULT.vault.azure.net/ 21 En nuestro docker-compose hemos definido la estructura de ejecución de nuestros servicio, en este caso solo tenemos un contenedor, donde le indicamos el network, puerto, nombre del contenedor, etc…\nEn este caso lo más importante son las propiedades volumes y environment. En el environment agregaremos nuestra url del Azure KeyVault, y en volumes lo que vamos a hacer es crear un volumen compartido donde copiaremos nuestra carpeta local de Azure para que podamos hacer sin ningún problema login con Azure CLI. Lo más importante es que aunque esta carpeta se suba no compromete nuestra seguridad pues no tiene nada vinculante.\nAhora modificaremos nuestro fichero .dockerfile para incluirle el Azure CLI y que podamos consumir la conexión hacia nuestro Azure KeyVault desde nuestro contenedor:\n1# https://hub.docker.com/_/microsoft-dotnet-core 2 FROM mcr.microsoft.com/dotnet/core/sdk:3.1 AS build 3 WORKDIR /app 4 5# copy csproj and restore as distinct layers 6 COPY *.csproj ./ 7 RUN dotnet restore 8 9# copy everything else and build app 10 COPY . ./ 11 RUN dotnet publish -c release -o out --no-restore 12 13# final stage/image 14 FROM mcr.microsoft.com/dotnet/core/aspnet:3.1 15 16# install azure cli 17ENV DEBIAN_FRONTEND noninteractive 18 19RUN apt-get update \\ 20 \u0026amp;\u0026amp; apt-get -y install --no-install-recommends apt-utils dialog 2\u0026gt;\u0026amp;1 \\ 21 # 22 # Verify git, process tools, lsb-release (common in install instructions for CLIs) installed 23 \u0026amp;\u0026amp; apt-get -y install git openssh-client iproute2 procps apt-transport-https gnupg2 curl lsb-release \\ 24 \u0026amp;\u0026amp; echo \u0026#34;deb [arch=amd64] https://packages.microsoft.com/repos/azure-cli/ $(lsb_release -cs) main\u0026#34; \u0026gt; /etc/apt/sources.list.d/azure-cli.list \\ 25 \u0026amp;\u0026amp; curl -sL https://packages.microsoft.com/keys/microsoft.asc | apt-key add - 2\u0026gt;/dev/null \\ 26 \u0026amp;\u0026amp; apt-get update \\ 27 \u0026amp;\u0026amp; apt-get install -y azure-cli; 28 29 30 WORKDIR /app 31 COPY --from=build /app/out . 32 ENTRYPOINT [\u0026#34;dotnet\u0026#34;, \u0026#34;azuretraining.dll\u0026#34;] Por último solo nos queda lanzar el siguiente comando para ejecutar nuestras aplicación en local «dockerizada» y «securizada»:\n1docker-compose up De esta forma tendremos nuestro proyecto completamente securizado pudiendo trabajar de forma fácil y sencilla, sin preocuparnos de que subamos información sensible a nuestro repositorio de código.\nDar las gracias a mi compañero @cmendibl3 por colaborar.\nSaludos!\n","date":"Feb 11, 2020","img":"https://manuss20.com/images/posts/Azure_keyVault_docker.jpg","permalink":"https://manuss20.com/2020/02/11/azure-keyvault-docker/","series":null,"tags":["Azure","Conatiner","Docker","DevOps","KeyVault","Cloud","Azure ARM"],"title":"Azure KeyVault + Docker"},{"categories":["Azure","Community","Netcoreconf","Netcore","Networking","Events","Workshops","Azure WebApps"],"content":"El pasado sábado 18 de enero tuvo lugar en Barcelona la primera edición de la Netcoreconf 2020.\nTuve la suerte de poder participar dando una sesión sobre Azure App Services: Age of Modernizing Apps. «Azure App Services nos ofrece un conjunto de herramientas para poder llevarnos nuestras aplicaciones Legacy al cloud sin tener que modificar gran cantidad de código, poder realizar CI/CD, escalar de forma incremental y automática, disponer de todo los logs y telemetría de forma centralizada pudiendo configurar alertas personalizadas, etc… En esta sesión aprenderemos a usar Azure App Services en un caso real con aplicaciones finales donde podremos repasar todos los aspectos y características más avanzadas a la hora de migrar nuestras apps al Cloud y ver cómo podemos automatizar todos nuestros procesos de una forma fácil y sencilla.»\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mi compañeros Robert Bermejo, Txema González y Adrián Díaz.\nSaludos!\n","date":"Jan 20, 2020","img":"https://manuss20.com/images/posts/logo_netcore_cartel.png","permalink":"https://manuss20.com/2020/01/20/eventos-netcoreconf-barcelona-2020/","series":null,"tags":["Azure","Community","Netcoreconf","Netcore","Networking","Event","Workshops","Azure WebApps"],"title":"[Eventos] Netcoreconf Barcelona 2020"},{"categories":["Azure","Azure Monitor","Azure Application Insight","Monitoring","Cloud"],"content":"La mayoría de las compañías tendrán una mezcla de PaaS, IaaS y SaaS, lo que crea un desafío único cuando se trata de monitoreo. El objetivo es poder solucionar las cosas de manera proactiva antes de que sus usuarios y su empresa se vean afectados por fallos inesperados o un rendimiento poco optimo. Para ello vamos a hablar en este articulo sobre Azure Monitor y Application Insights:\nAzure Monitor Azure Monitor es un servicio de Azure que proporciona supervisión del rendimiento y la disponibilidad para aplicaciones y servicios en Azure, en otros entornos en la nube o en el entorno local. Azure Monitor recopila datos de varios orígenes en una plataforma de datos común en la que se pueden analizar las tendencias y las anomalías. Las características enriquecidas de Azure Monitor ayudan a identificar y responder rápidamente ante situaciones críticas que pueden afectar a la aplicación.\nLas características de Azure Monitor que están habilitadas automáticamente, como la recopilación de métricas y los registros de actividad, que se proporcionan sin costé alguno. Es importante destacar que no existe una versión local de Azure Monitor ya que es un servicio en la nube escalable que procesa y almacena grandes cantidades de datos, aunque Azure Monitor puede supervisar los recursos que están en el entorno local y en otras nubes.\nApplication Insights Application Insights es una característica de Azure Monitor que es un servicio de Application Performance Management (APM) extensible para desarrolladores y profesionales de DevOps. Úselo para supervisar las aplicaciones en directo. Detectará automáticamente anomalías en el rendimiento e incluye eficaces herramientas de análisis que le ayudan a diagnosticar problemas y a saber lo que hacen realmente los usuarios con la aplicación. Está diseñado para ayudarle a mejorar continuamente el rendimiento y la facilidad de uso. Funciona con diversas aplicaciones y en una amplia variedad de plataformas, como .NET, Node.js o Java EE, hospedadas en el entorno local, de forma híbrida o en cualquier nube pública. Se integra con el proceso de DevOps y tiene puntos de conexión a numerosas herramientas de desarrollo. Puede supervisar y analizar la telemetría de aplicaciones móviles mediante la integración con Visual Studio App Center.\nSaludos!\n","date":"Nov 22, 2019","img":"https://manuss20.com/images/posts/monitorizacion.jpg","permalink":"https://manuss20.com/2019/11/22/monitorizaci%C3%B3n-de-aplicaciones-en-azure/","series":null,"tags":["Azure","Azure Monitor","Azure Application Insight","Monitoring","Cloud"],"title":"Monitorización De Aplicaciones en Azure"},{"categories":["Azure","Netcoreconf","Netcore","Networking","Community","Events","Workshops"],"content":"El pasado sábado 29 de septiembre tuvo lugar en Galicia la segunda edición de la Netcoreconf.\nTuve la suerte de poder participar dando una sesión sobre Azure SignalR (SignalR \u0026amp; The power of the Lightning 🌩), podéis encontrarlo en: https://github.com/netcoreconf/NetcoreconfGAL_2019/blob/master/Track%202/02%20-%20SignalR%20%26%20The%20power%20of%20the%20Lightning%20🌩.pptx\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mi compañeros Robert Bermejo y Txema González. Gracias a @David y @Alberto por todo el esfuerzo y el Centro socio Cultural de Santa Marta por ceder unas magníficas instalaciones de forma desinteresada. ¡Nos vemos en la próxima edición! Saludos!\n","date":"Sep 26, 2019","img":"https://manuss20.com/images/posts/logo_netcore_cartel.png","permalink":"https://manuss20.com/2019/09/26/eventos-netcoreconf-galicia-2019/","series":null,"tags":["Azure","Netcoreconf","Netcore","Networking","Community","Event","Workshops"],"title":"[Eventos] Netcoreconf Galicia 2019"},{"categories":["Azure","Azure DevOps","DevOps","Beginners","Global DevOps Bootcamp","LifeCycle","Community","Cloud","Networking","Events","Workshops"],"content":"El pasado sábado 15 de junio tuvo lugar en Barcelona la última edición del Global DevOps Bootcamp. Tuve la suerte de poder participar dando una sesión sobre DevOps (LifeCycle of DevOps_Complete Beginners Training) junto a mi compañero Txema González, podéis encontrarlo en:\nhttps://github.com/gdbc-barcelona/2019\nhttps://github.com/gdbc-barcelona/2019/tree/master/Track%201/02%20-%20LifeCycle%20of%20DevOps%20Complete%20Beginners%20Training\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mi compañeros Robert Bermejo y gracias a everis por ceder unas magníficas instalaciones de forma desinteresada. ¡Nos vemos en la próxima edición!\nSaludos!\n","date":"Jun 15, 2019","img":"https://manuss20.com/images/posts/devops-bootcamp.jpg","permalink":"https://manuss20.com/2019/06/15/eventos-global-devops-bootcamp/","series":null,"tags":["Azure","Azure DevOps","DevOps","Beginners","Global DevOps Bootcamp","LifeCycle","Community","Cloud","Networking","Event","Workshops"],"title":"[Eventos] Global DevOps Bootcamp"},{"categories":["Azure","Azure Digital Twins","IoT","Templates","Cloud","Azure ARM"],"content":"Digital Twins es una de las tecnologías que está revolucionando el sector industrial, gracias a ella podemos simplificar el proceso de Transformación Digital. Azure Digital Twins es una plataforma que proporciona a las organizaciones la base para poder realizarla, pudiendo simplificar el proceso de digitalización creando experiencias escalables, correlacionando los datos de origen digital con los del mundo físico. En este artículo, nos centraremos en ver como podemos realizar el despliegue de nuestra solución Digital Twins y consultar en tiempo real el estado de los dispositivos conectados.\nAzure Digital Twins es un nuevo servicio de Azure con el que podremos que crea modelos completos del entorno físico, pudiendo crear grafos de inteligencia espacial para modelar las relaciones y las interacciones entre personas, espacios y dispositivos.\nEn este articulo nos centraremos en como poder crear un nuevo servicio de Azure Digital Twins a través de una suscripción de Azure, consultar los datos de los sensores simulados y comprobar si la habitación esta disponible para su uso usando los datos extraídos de los sensores.\nPara la creación de la instancia de Digital Twins en nuestra suscripción de Azure deberemos de entrar en el portal de Azure -\u0026gt; Crear un Recurso -\u0026gt; Digital Twins\nTened en cuenta que solo se puede crear una única instancia de Digital Twins por suscripción.\nUna vez que ya tenemos aprovisionada la instancia del servicio Digital Twins podremos ver la siguiente información general, donde encontraremos la url del Management API. Desde esta url nos mostrará todas las capacidades que ha aprovisionado el servicio de Digital Twins y que podremos usar.\nLa url que nos muestra por defecto el servicio es la que contiene toda la información de la API REST de Azure Digital Twins que se aplica a la instancia, normalmente suele tener el siguiente formato:\nhttps://yourDigitalTwinsName.yourLocation.azuresmartspaces.net/management/swagger\nSin embargo, para poder tener acceso a la instancia del servicio deberemos de modificar la url:\nhttps://yourDigitalTwinsName.yourLocation.azuresmartspaces.net/management/api/v1.0/\n(esta url podemos guardarla en un fichero de texto para poder usarlo en los pasos posteriores)\nAhora deberemos de definir los permisos para nuestra instancia de Digital Twins, por lo que deberemos de entrar en el portal de Azure -\u0026gt; Registros de Aplicaciones -\u0026gt; Nuevo registro de aplicaciones.\nLe asignaremos un nombre, seleccionaremos para este ejemplo la opción solo las cuentas de este directorio organizativo del aparatado Tipos de Cuentas compatibles he informaremos la url de nuestra web. Por último, registraremos la aplicación.\nUna vez que hemos registrado la aplicación podemos revisar en la información general el id de aplicación, Id de directorio (esta información podemos guardarla en un fichero de texto para poder usarlo en los pasos posteriores para provisionar y consultar los datos de la instancia). Debemos irnos al apartado Permisos de API y añadir permisos.\nMuy importante que cuando añadamos el permiso busquemos en API usadas en mi organización con el nombre Azure Digital Twins. Una vez seleccionado le daremos los permisos de Lectura/Escritura y para finalizar deberemos de Conceder Permisos.\nPor último, deberemos de ir a portal de Azure -\u0026gt; Azure Active Directory -\u0026gt; Propiedades donde podremos consultar el Id de Directorio (esta información podemos guardarla en un fichero de texto para poder usarlo en los pasos posteriores).\nCon estos pasos ya tenemos provisionado nuestro servicio de Azure Digital Twins para poder trabajar con él. Ahora vamos a usar un ejemplo ya preparado por el equipo de Microsoft que podréis descargar desde GitHub en el siguiente enlace (https://github.com/Azure-Samples/digital-twins-samples-csharp/) para poder «provisionar una estructura completa».\nUna vez que nos descarguemos el ejemplo y lo hayamos descomprimido en una carpeta en nuestro sistema, por ejemplo C:\\samples\\digital-twins-sample, os recomendaría abrirlo con Visual Studio Code y de esta forma tener una visión del proyecto además de la terminal en una sola ventana.\nCuando entramos en la carpeta descomprimida observamos dos proyectos netcore que son occupancy-quickstart (nos permitirá consultar los datos en tiempo real) y device-connectivity (nos permitirá aprovisionar nuestro servicio con sensores simulados).\nNos centraremos primero en el aprovisionamiento, para ello recordad que en los pasos anteriores os pedía que guardarais cierta información, ahora es el momento de usarla. Iremos a la ruta occupancy-quickstart\\src\\ appSettings.json\nUna vez guardemos el fichero con los datos solicitados, desde consola realizaremos los siguientes comandos:\n cd occupancy-quickstart\\src dotnet restore dotnet run ProvisionSample  La primera vez que lo hacemos nos pedirá que nos loguemos en una url con un código determinado que se genera aleatoriamente para identificar el servicio. Al ser un servicio que está actualmente en preview se solicitara cada 24h.\nUna vez que hemos autorizado el servicio a través de los pasos anteriores, empezara a generar el aprovisionamiento que se ha definido concretamente para tener más información es en el archivo digital-twins-sample\\occupancy-quickstart\\src\\actions\\ provisionSample.yaml\nSabremos que ha finalizado cuando encontremos al final la instrucción «Completed Provisioning» además ahora deberemos de buscar la clave ConnectionString y copiaremos su valor en un archivo de texto para usarlo en el proyecto device-connectivity para poder observar los datos.\nDeberemos de ir al fichero digital-twins-sample\\device-connectivity\\appsettings.json y copiar el valor que hemos obtenido en el aprovisionamiento del paso anterior, como se puede observar aquí es donde también definimos los dispositivos que queremos observar:\nAhora desde una terminar realizaremos los siguientes comandos:\n cd device-connectivity dotnet restore dotnet run​  Podemos observar como nos devuelve los resultados del sensor en tiempo real. Para poder tener una visión completa de Azure Digital Twins abrimos una nueva terminal en paralelo y lanzamos los siguientes comandos:\n cd occupancy-quickstart\\src dotnet run GetAvailableAndFreshSpaces  Si colocamos las dos ventanas en el escritorio podremos ver que en la primera estamos obteniendo los datos en tiempo real de los sensores y en la otra terminal estamos tratando esos datos y mostrándonos los espacios disponibles con aire fresco que tenemos en ese momento. Esta lógica se ha definido en el fichero digital-twins-sample\\occupancy-quickstart\\src\\actions\\userDefinedFunctions\\availability.js\nPara poder tener una visión completa de todo los dispositivos, estancias que tenemos aprovisionadas podemos usar un visor de grafos espacial (https://github.com/Azure/azure-digital-twins-graph-viewer) con el que podremos ver de una forma más visual nuestro aprovisionamiento que hemos realizado en este ejemplo:\nConclusiones Sinceramente pienso que el nuevo servicio de Azure Digital Twins es muy útil para poder representar el mundo físico y sus muchas relaciones, con ello puede ayudar a simplificar el modelado, procesamiento de datos, control de eventos y el seguimiento de dispositivos IoT. Con ello el poder llevar la transformación digital a las diferentes industrias se puede conseguir de una forma segura, fácil y sobre todo unificada.\nPor otro lado, debemos de tener en cuenta que actualmente es un servicio en preview y por tanto tiene algunas limitaciones. Aunque se espera que para final de año se libere completamente.\nSaludos!\n","date":"Jun 1, 2019","img":"https://manuss20.com/images/posts/Azure-digital-twins.jpg","permalink":"https://manuss20.com/2019/06/01/azure-digital-twins/","series":null,"tags":["Azure","Azure Digital Twins","IoT","Templates","Cloud","Azure ARM"],"title":"Azure Digital Twins"},{"categories":["Azure","Azure ARM","DevOps","Deploy","Templates","Cloud"],"content":"Las plantillas de Azure ARM (Azure Resource Manager) permite aprovisionar las aplicaciones usando una plantilla declarativa. En una sola plantilla, se pueden implementar varios servicios junto con sus dependencias. Se usa la misma plantilla para implementar repetidamente la aplicación durante cada etapa de su ciclo de vida.\nEsto nos ayudará a poder desplegar todos los componentes de nuestra solución, de una única vez, en un grupo de recursos concreto. De esta forma simplificamos la administración de éstos entre los diferentes entornos. Por ejemplo: desarrollo, preproducción y producción.\nVamos a crear un ejemplo sencillo de ARM siguiendo estos pasos:\nAbrimos Visual Studio y seleccionamos un proyecto del tipo «Azure Resource Group»:\nCuando estamos creando nuestro proyecto nos solicitara elegir un tipo de plantilla, para este caso concreto seleccionaremos la plantilla del tipo “Web App”:\nUna vez seleccionado el tipo de plantilla nos creará nuestra solución que tendrá el siguiente aspecto:\nEl esquema que aparece en la foto anterior se puede apreciar 3 ficheros importantes:\n Deploy-AzureResourceGroup.ps1: Script de despliegue. WebSite.json: Fichero donde se encuentran definido los componentes a desplegar. WebSite.parameters.json: Fichero que contiene los parámetros de configuración.  Dentro del fichero WebSite.json podemos observar el esquema de la plantilla que hemos creado. Por defecto incluye un montón de paramentos adicionales, aunque podemos simplificarlo con los siguientes elementos esenciales:\n1{ 2 \u0026#34;$schema\u0026#34;: \u0026#34;https://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#\u0026#34;, 3 \u0026#34;contentVersion\u0026#34;: \u0026#34;1.0.0.0\u0026#34;, 4 \u0026#34;parameters\u0026#34;: { }, 5 \u0026#34;variables\u0026#34;: { }, 6 \u0026#34;resources\u0026#34;: { }, 7 \u0026#34;outputs\u0026#34;: { }, 8}  $Schema: Contiene el esquema JSON que se usará para la plantilla ARM. contentVersion: Especifica la versión de la plantilla que estás usando. Por defecto se asignará automáticamente el - valor inicial de 1.0.0.0 Parameters: Son los parámetros que se pasan a la ejecución de la plantilla. Variables: Variables para poder reutilizarlas a lo largo de la plantilla. Resources: Epecifica los componentes que se van a desplegar. Outputs: Es la salida de resultados de la plantilla.  Una vez tengamos la plantilla finalizada, procederemos a realizar el despliegue. Éste se puede realizar desde Visual Studio o desde PowerShell (en nuestro caso lo haremos desde Visual Studio):\nDesde la solución hacemos click derecho sobre el proyecto -\u0026gt; Deploy -\u0026gt; new (tal y como vemos en la siguiente imagen):\nNos solicitará algunos parámetros indicando nuestra suscripción de Azure, y nos preguntará si queremos crear un nuevo grupo de recursos o crearlo en uno ya existente:\nNos indicará si falta algún dato obligatorio para la ejecución:\nUna vez lanzado el deploy, podremos ver el output de resultados para asegurarnos que todo ha ido correctamente:\nUna vez nos diga que está completado, podremos ir a nuestra suscripción de Azure, para comprobar que se han creado los recursos correctamente:\nConclusiones:  Permite desplegar, gestionar y controlar todos los recursos para su solución como un solo grupo, en lugar de - manejar estos recursos de manera individual. Reutilizar esa implementación varias veces a lo largo de todo el ciclo de vida del desarrollo, y tener la - confianza de que sus recursos se despliegan correctamente. Podemos definir las dependencias entre los recursos para que se desplieguen en el orden correcto. Nos permite poder aplicar etiquetas a los recursos para organizarlos lógicamente en la suscripción. Nos ayuda a gestionar nuestra infraestructura a través de plantillas declarativas en lugar de scripts. Unificar la visualización de la facturación de tu organización mediante la visualización de los gastos para un - grupo de recursos que comparten la misma etiqueta.  Saludos!\n","date":"May 7, 2019","img":"https://manuss20.com/images/posts/Azure-ARM.jpg","permalink":"https://manuss20.com/2019/05/07/azure-arm-templates/","series":null,"tags":["Azure","Azure ARM","DevOps","Deploy","Templates","Cloud"],"title":"Azure ARM Templates"},{"categories":["Azure","Global Azure Bootcamp","Community","Cloud","Networking","Events","Workshops"],"content":"El pasado sábado 27 de abril tuvo lugar en Barcelona la última edición del Global Azure Bootcamp.\nTuve la suerte de poder participar dando una sesión sobre Azure digital Twins, podéis encontrarlo en:\nhttps://github.com/GABSpain/Global-Azure-Bootcamp-2019\nhttps://github.com/GABSpain/Global-Azure-Bootcamp-2019/tree/master/Barcelona/Track%201/03%20-%20Azure%20Digital%20Twins%20un%20mundo%20paralelo\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mis compañeros Robert Bermejo, Adrían Diaz, Nacho Fanjul, y gracias a Ironhack por ceder unas magníficas instalaciones de forma desinteresada.\n¡Nos vemos en la próxima edición!\nSaludos!\n","date":"Apr 28, 2019","img":"https://manuss20.com/images/posts/Logo_GAB_2019.png","permalink":"https://manuss20.com/2019/04/28/eventos-global-azure-bootcamp-2019/","series":null,"tags":["Azure","Global Azure Bootcamp","Community","Cloud","Networking","Event","Workshops"],"title":"[Eventos] Global Azure Bootcamp 2019"},{"categories":["Azure","Integration","Azure Logic Apps","Global Integration Bootcamp","Community","Cloud","Networking","Events","Workshops"],"content":"El pasado sábado 30 de marzo tuvo lugar en Barcelona la última edición del Global Integration Bootcamp.\nTuve la suerte de poder participar dando una sesión sobre Logic Apps (Process automation with Logic Apps power), podéis encontrarlo en: https://github.com/Manuss20/GIB2019\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mi compañeros Robert Bermejo y gracias a everis por ceder unas magníficas instalaciones de forma desinteresada. ¡Nos vemos en la próxima edición!\nSaludos!\n","date":"Mar 31, 2019","img":"https://manuss20.com/images/posts/GIB2019.png","permalink":"https://manuss20.com/2019/03/31/eventos-global-integration-bootcamp-2019-barcelona/","series":null,"tags":["Azure","Integration","Azure Logic Apps","Global Integration Bootcamp","Community","Cloud","Networking","Event","Workshops"],"title":"[Eventos] Global Integration Bootcamp 2019 Barcelona"},{"categories":null,"content":"My name is Manuel Sánchez, I´m Technical Manager \u0026amp; Azure Evangelist at NTT Data \u0026amp; Microsoft Azure MVP. Passionate about new technologies, highlighting Netcore , Microsoft Azure, Xamarin, IA, Bots and ASP.NET. Passionate about team management. I contribute to the developers community writing articles in my personal blog and giving speeches in numerous events. I am also one of the Netcoreconf \u0026amp; CATzure leads.\nThese are some of the events that I also participate in and organize with other colleagues from the community:\n Global Azure Bootcamp Global DevOps Bootcamp Global Integration Bootcamp Global AI Bootcamp Azure Day Azure Summer of Experts  I love being able to talk about technology and share my knowledge with the rest of the community. Some of my hobbies are basketball, running, playing the guitar and of course beer!\n","date":"Feb 28, 2019","img":"","permalink":"https://manuss20.com/about/","series":null,"tags":null,"title":"About Me"},{"categories":["Azure","Azure Functions","Functions","Serverless","SaaS","Cloud"],"content":"Azure Functions es una de las nuevas soluciones que nos ofrece Azure para ejecutar fácilmente pequeños fragmentos de código, o «funciones», en la nube como servicio FaaS (Arquitectura Serverless). Podemos escribir el código que necesita para el problema en cuestión, sin preocuparse de toda la aplicación o la infraestructura para ejecutarlo.\nAdemás nos permite utilizar el lenguaje de desarrollo que prefiera, como C#, F#, Node.js, Java o PHP, etc… Por su puesto, sólo pagamos por el tiempo que nuestras funciones se están ejecutando o, lo que es lo mismo, por los recursos de Azure que necesita para su ejecución.\nAlgunos de los ejemplos donde usar las functions:\n Trigger desde una Queue de Azure Storage Trigger desde una Queue de Azure Service Bus Trigger desde un Topic de Azure Service Bus Webhook Petición HTTP Ejecución programada por tiempo Trigger desde un Blob de Azure Storage Trigger desde un Event Hub   Una plataforma para cargar el código de la aplicación, ejecutar y administrar la aplicación sin tener que pensar en configurar ningún servido\n Podemos comprarlo con los servicios de Windows en una arquitectura OnPremises, Azure Functions nos cubre un gran número de casos de uso en llamadas a back-end de nuestras aplicaciones. Ya que este modelo se basa en ejecutar código de backend sin administrar los servidores o las aplicaciones, el escalado horizontal es completamente automático y gestionado por el proveedor donde hemos alojado nuestro código.\nSi nos ponemos manos a la obra para definir una arquitectura Cloud, será un gran aliado para resolverá vuestros problemas.\nSaludos!\n","date":"Feb 9, 2019","img":"https://manuss20.com/images/posts/The-power-of-Azure-Functions.jpg","permalink":"https://manuss20.com/2019/02/09/the-power-of-azure-functions/","series":null,"tags":["Azure","Azure Functions","Functions","Serverless","SaaS","Cloud"],"title":"The Power of Azure Functions"},{"categories":["Azure","Community","Netcoreconf","Netcore","Networking","Events","Workshops"],"content":"El pasado sábado 26 de enero tuvo lugar en Barcelona la primera edición de la Netcoreconf.\nTuve la suerte de poder participar dando una sesión junto a Robert Bermejo sobre Azure Maps (Llevando al «geospacio» nuestras aplicaciones con Azure Maps), podéis encontrarlo en: https://github.com/Manuss20/netcoreconf2019\nFue una gran jornada de grandes sesiones y de grandes asistentes que vinieron con muchas ganas de aprender, compartir y de ayudar.\nGracias a la organización de la que soy participe junto a mi compañeros Robert Bermejo y Txema González. Gracias a Ironhack por ceder unas magníficas instalaciones de forma desinteresada. ¡Nos vemos en la próxima edición!\nSaludos!\n","date":"Jan 27, 2019","img":"https://manuss20.com/images/posts/logo_netcore_cartel.png","permalink":"https://manuss20.com/2019/01/27/eventos-netcoreconf-barcelona-2019/","series":null,"tags":["Azure","Community","Netcoreconf","Netcore","Networking","Event","Workshops"],"title":"[Eventos] Netcoreconf Barcelona 2019"},{"categories":["Azure","Community","Netcoreconf","Netcore","Networking","Events","Workshops"],"content":"Lo último en tecnologías Microsoft y mucho más con los mejores expertos. Donde podrás aprender, compartir y hacer networking. Asistiendo a diversas Conferencias y Workshops. Hablaremos sobre NetCore, Azure, Xamarin, IA, Big Data. ¿A que estas esperando?\nNetcoreconf 2019 realizará el primer evento a nivel estatal dedicado exclusivamente al sector del desarrollo y consultoría que busca descubrir y dar a conocer las nuevas tecnologías de vanguardia y crear vínculos estratégicos que generen sinergias conjuntas entre los profesionales del sector, empresas e instituciones.\n¿Te vas a perder la mayor evento sobre las tendencias tecnologías de este año? ¡A que estas esperando a conseguir tu entrada!!\nhttps://www.eventbrite.es/e/entradas-netcoreconf-barcelona-2019-52316068770?ref=eios\u0026amp;aff=eios\nSaludos!\n","date":"Jan 3, 2019","img":"https://manuss20.com/images/posts/logo_netcore_cartel.png","permalink":"https://manuss20.com/2019/01/03/netcoreconf/","series":null,"tags":["Azure","Community","Netcoreconf","Netcore","Networking","Event","Workshops"],"title":"Netcoreconf"},{"categories":["Azure","WenApps","Azure Web App","Terraforms","Cloud","Containers","Docker","IaC"],"content":"Una Web App es un servicio en el cual ponen a disposición una plataforma previamente configurada para alojar aplicaciones web, APIs tipo REST y Back Ends para aplicaciones móviles. Tales aplicaciones pueden estar desarrolladas en cualquier lenguaje conocido como puede ser .Net, Java, Ruby, NodeJS, Python, PHP, entre otros.\nUna de las ventajas de estos servicios es que permiten escalar el tamaño de recursos que la Web App necesita con facilidad y le evitan a los desarrolladores de software y empresas tener que instalar y administrar un entorno completo de sistema operativo, base de datos e intérpretes de lenguajes.\nActualmente la tendencia en los desarrollos web modernos está usando Docker containers para “dockerizar” sus aplicaciones. El uso en entornos donde se está creando una arquitectura de microservicios, es probable que esos despliegues los estén haciendo con herramientas de automatización como pueden ser AKS o Terraform.\nEn este ejemplo nos centraremos en la automatización con Terraform. Actualmente si deseamos usar Azure Web Apps como host de nuestro contenedor, la documentación de Terraform no nos da toda la información necesaria para poder hacerlo. Por ello a continuación os facilito un ejemplo de cómo poder configurarlo:\n1# Use the Azure Resource Manager Provider 2provider \u0026#34;azurerm\u0026#34; { 3 version = \u0026#34;~\u0026gt; 1.15\u0026#34; 4} 5 6# Create a new Resource Group 7resource \u0026#34;demo_resource_group\u0026#34; \u0026#34;group\u0026#34; { 8 name = \u0026#34;webapp-containers-demo\u0026#34; 9 location = \u0026#34;westeurope\u0026#34; 10} 11 12# Create an App Service Plan with Linux 13resource \u0026#34;azurerm_app_service_plan\u0026#34; \u0026#34;appserviceplan\u0026#34; { 14 name = \u0026#34;${demo_resource_group.group.name}-plan\u0026#34; 15 location = \u0026#34;${demo_resource_group.group.location}\u0026#34; 16 resource_group_name = \u0026#34;${demo_resource_group.group.name}\u0026#34; 17 18 # Define Linux as Host OS 19 kind = \u0026#34;Linux\u0026#34; 20 21 # Choose size 22 sku { 23 tier = \u0026#34;Standard\u0026#34; 24 size = \u0026#34;S1\u0026#34; 25 } 26 27 properties { 28 reserved = true # Mandatory for Linux plans 29 } 30} 31 32# Create an Azure Web App for Containers in that App Service Plan 33resource \u0026#34;azurerm_app_service\u0026#34; \u0026#34;dockerapp\u0026#34; { 34 name = \u0026#34;${demo_resource_group.group.name}-dockerapp\u0026#34; 35 location = \u0026#34;${demo_resource_group.group.location}\u0026#34; 36 resource_group_name = \u0026#34;${demo_resource_group.group.name}\u0026#34; 37 app_service_plan_id = \u0026#34;${azurerm_app_service_plan.appserviceplan.id}\u0026#34; 38 39 # Do not attach Storage by default 40 app_settings { 41 WEBSITES_ENABLE_APP_SERVICE_STORAGE = false 42 43 /* 44 # Settings for private Container Registires 45 DOCKER_REGISTRY_SERVER_URL = \u0026#34;\u0026#34; 46 DOCKER_REGISTRY_SERVER_USERNAME = \u0026#34;\u0026#34; 47 DOCKER_REGISTRY_SERVER_PASSWORD = \u0026#34;\u0026#34; 48 */ 49 } 50 51 # Configure Docker Image to load on start 52 site_config { 53 linux_fx_version = \u0026#34;DOCKER|appsvcsample/static-site:latest\u0026#34; 54 always_on = \u0026#34;true\u0026#34; 55 } 56 57 identity { 58 type = \u0026#34;SystemAssigned\u0026#34; 59 } 60} Para asegurarnos que se configura correctamente debe de prestar especial atención a que las siguientes propiedades estén correctamente definidas en su archivo de configuración, de lo contrario no funcionará correctamente el despliegue:\n1reserved = true 2WEBSITES_ENABLE_APP_SERVICE_STORAGE = false Saludos!\n","date":"Dec 20, 2018","img":"https://manuss20.com/images/posts/despliege_WebApps_Azure_Terraforms.jpg","permalink":"https://manuss20.com/2018/12/20/despliegue-azure-web-app-para-contenedores-con-terraform/","series":null,"tags":["Azure","WepApps","Azure Web App","Terraforms","Cloud","Containers","Docker","IaC"],"title":"Despliegue Azure Web App Para Contenedores Con Terraform"},{"categories":["Azure","Azure Automation","Azure Event Grid","Cloud"],"content":"Azure Event Grid es una plataforma de enrutamiento de eventos inteligentes administrados que le permite reaccionar en tiempo real a los cambios que están ocurriendo en sus aplicaciones alojadas en Azure o en cualquier recurso de Azure que usted posea.\nActualmente para recibir una notificación de un cambio de etapa en un recurso, como un nuevo registro en una base de datos, o cuando alguien crea una Máquina Virtual, generalmente piensa en un sondeo programado o continuo. Eso significa que estamos constantemente consumiendo potencia de cómputo (CPU y recursos de red) para monitorear todos estos cambios.\nPero ahora con Azure Event Grids esto puede cambiar. ¡Podemos hacer que tus aplicaciones envíen eventos cuando se produce un cambio de estado, al igual que las notificaciones automáticas! Esos solo se activan una vez que se envían los eventos procesables.\nCualquier componente que esté interesado en esos eventos puede suscribirse y recibir notificaciones sin sondeo, reduciendo los recursos y costes.\n Event Grid nos permite poder integrarlo con cualquier servicio o aplicaciones debido a que todo se basa en HTTP, Event Grid puede integrarse virtualmente con cualquier servicio o aplicación. Aquí entra en acción las ventajas de Event Grid, la simplificación de operaciones y en la automatización de la seguridad a través de una aplicación de políticas más sencilla, expandiendo los escenarios sin servidor y sin fuentes de eventos. Permitiendo una mejor comunicación e integración entre sus servicios y aplicaciones basados en eventos.\n Event Grid, gestiona los eventos de Azure incorporados de los servicios de Azure, así como los eventos personalizados de sus aplicaciones y los publica en tiempo real. Puede escalar y manejar dinámicamente millones de eventos cada segundo y Azure ofrece 99.99 de disponibilidad (Acuerdos de nivel de servicio ) para cargas de trabajo en producción.\nA medida que se reciben los eventos, Event Grid facilita la activación de acciones programáticas a través de los controladores de eventos, como pueden ser Azure Automation, Event Hubs, Azure Functions, Azure Logic Apps, ect.\nPodéis encontrar algunos ejemplos acerca de Event Grid en: https://docs.microsoft.com/es-es/azure/event-grid/monitor-virtual-machine-changes-event-grid-logic-app\nSaludos!\n","date":"Nov 18, 2018","img":"https://manuss20.com/images/posts/azureeventgrid.jpg","permalink":"https://manuss20.com/2018/11/18/azure-event-grid/","series":null,"tags":["Azure","Azure Event Grid","Event Grid","Processes","Cloud"],"title":"Azure Event Grid"},{"categories":["Azure","Events","Community","SaaS","PaaS","IaaS","Cloud"],"content":"Otro año más se celebra el evento de comunidad referencia sobre Microsoft Azure en más 263 localizaciones de todo el mundo.\nEl Global Azure Bootcamp es un evento organizado por la comunidad y patrocinado por Microsoft y partners locales de tecnologías Microsoft orientado a ser uno de los foros técnicos más importantes de la plataforma de Cloud Computing de Microsoft.\nEste año tenemos confirmadas 263 localizaciones, en las que destacar en España:\n Barcelona Madrid Palma de Mallorca Santander  ","date":"Apr 26, 2018","img":"https://manuss20.com/images/posts/GAB2018.png","permalink":"https://manuss20.com/2018/04/26/global-azure-bootcamp-2018/","series":null,"tags":["Azure","Event","Community","SaaS","PaaS","IaaS","Cloud"],"title":"Global Azure Bootcamp 2018"},{"categories":null,"content":"","date":"Jan 1, 0001","img":"","permalink":"https://manuss20.com/offline/","series":null,"tags":null,"title":"Offline"}]